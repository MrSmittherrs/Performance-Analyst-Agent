"""
NeoLoad Results Parser: Parses NeoLoad results into structured JSON.

Supports two input formats, auto-detected from the path:
- NeoLoad HTML reports (summary.html + transactions.html) — generated by NeoLoad after a run
- NeoLoad CSV/XML exports (semicolon-separated by default)

Usage:
    python tools/neoload_parser.py <results_path> --project "Project Name" --date "2026-02-16"
    python tools/neoload_parser.py <results_path> --project "Project Name" --date "2026-02-16" --label "Run 1"
    python tools/neoload_parser.py results.csv  --project "Project Name" --date "2026-02-16" --separator ";"
"""

import os
import sys
import json
import csv
import glob
import argparse
import re
from datetime import datetime
from pathlib import Path
from html.parser import HTMLParser

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from dotenv import load_dotenv


# ---------------------------------------------------------------------------
# CSV parsing helpers
# ---------------------------------------------------------------------------

TRANSACTION_COLUMN_ALIASES = {
    "element":       ["element", "transaction", "name", "transaction name", "element name"],
    "user_path":     ["user path", "userpath", "virtual user", "population"],
    "parent":        ["parent", "parent element", "container"],
    "count":         ["count", "executions", "iteration", "total"],
    "min":           ["min", "minimum", "min response time", "min (ms)"],
    "avg":           ["avg", "average", "mean", "avg response time", "avg (ms)"],
    "max":           ["max", "maximum", "max response time", "max (ms)"],
    "perc_50":       ["perc 50", "p50", "50th percentile", "median", "percentile 50"],
    "perc_90":       ["perc 90", "p90", "90th percentile", "percentile 90"],
    "perc_95":       ["perc 95", "p95", "95th percentile", "percentile 95"],
    "perc_99":       ["perc 99", "p99", "99th percentile", "percentile 99"],
    "success":       ["success", "passed", "success count"],
    "success_rate":  ["success rate", "success rate (%)", "pass rate", "success %"],
    "failure":       ["failure", "failed", "failure count", "errors"],
    "failure_rate":  ["failure rate", "failure rate (%)", "fail rate", "error rate", "failure %"],
}

MONITOR_COLUMN_ALIASES = {
    "timestamp": ["timestamp", "time", "date", "datetime"],
    "host":      ["host", "server", "machine", "hostname", "monitored machine"],
    "metric":    ["metric", "counter", "monitor", "counter name"],
    "value":     ["value", "avg", "average", "result"],
}


def detect_separator(file_path, candidates=(";", ",", "\t", "|")):
    with open(file_path, "r", encoding="utf-8-sig") as f:
        sample_lines = [f.readline() for _ in range(5)]
    sample_lines = [l for l in sample_lines if l.strip()]
    best_sep, best_consistency = ";", 0
    for sep in candidates:
        counts = [len(l.split(sep)) for l in sample_lines]
        if counts and counts[0] > 1:
            consistency = counts.count(counts[0])
            if consistency > best_consistency or (consistency == best_consistency and counts[0] > best_consistency):
                best_consistency = consistency
                best_sep = sep
    return best_sep


def normalize_col(col_name):
    return col_name.strip().lower().replace("_", " ").replace("-", " ")


def map_columns(headers, alias_map):
    mapping = {}
    normalized = [normalize_col(h) for h in headers]
    for standard_name, aliases in alias_map.items():
        for i, norm in enumerate(normalized):
            if norm in aliases:
                mapping[standard_name] = i
                break
    return mapping


def safe_float_csv(value, default=0.0):
    if value is None:
        return default
    try:
        cleaned = str(value).strip().replace(",", ".").replace("%", "")
        return float(cleaned) if cleaned else default
    except (ValueError, TypeError):
        return default


def parse_transaction_csv(file_path, separator=None):
    if separator is None:
        separator = detect_separator(file_path)
    transactions = []
    with open(file_path, "r", encoding="utf-8-sig") as f:
        reader = csv.reader(f, delimiter=separator)
        headers = next(reader)
        col_map = map_columns(headers, TRANSACTION_COLUMN_ALIASES)
        if "element" not in col_map and "avg" not in col_map:
            return None
        for row in reader:
            if not row or all(c.strip() == "" for c in row):
                continue
            txn = {}
            for field, idx in col_map.items():
                if idx < len(row):
                    if field in ("element", "user_path", "parent"):
                        txn[field] = row[idx].strip()
                    else:
                        txn[field] = safe_float_csv(row[idx])
            if txn.get("element"):
                transactions.append(txn)
    return transactions


def parse_monitor_csv(file_path, separator=None):
    if separator is None:
        separator = detect_separator(file_path)
    monitors = []
    with open(file_path, "r", encoding="utf-8-sig") as f:
        reader = csv.reader(f, delimiter=separator)
        headers = next(reader)
        col_map = map_columns(headers, MONITOR_COLUMN_ALIASES)
        if "host" not in col_map and "metric" not in col_map:
            return None
        for row in reader:
            if not row or all(c.strip() == "" for c in row):
                continue
            entry = {}
            for field, idx in col_map.items():
                if idx < len(row):
                    if field in ("host", "metric", "timestamp"):
                        entry[field] = row[idx].strip()
                    else:
                        entry[field] = safe_float_csv(row[idx])
            if entry.get("host") or entry.get("metric"):
                monitors.append(entry)
    return monitors


def discover_csv_files(results_path):
    results_path = Path(results_path)
    discovered = {"transaction_files": [], "monitor_files": [], "raw_files": [], "other_files": []}
    if results_path.is_file():
        patterns = [str(results_path)]
    else:
        patterns = [
            str(results_path / "**" / "*.csv"),
            str(results_path / "**" / "*.CSV"),
            str(results_path / "*.csv"),
        ]
    csv_files = set()
    for pattern in patterns:
        csv_files.update(glob.glob(pattern, recursive=True))
    for fp in sorted(csv_files):
        fname = os.path.basename(fp).lower()
        if any(kw in fname for kw in ["monitor", "infra", "counter", "server", "resource"]):
            discovered["monitor_files"].append(fp)
        elif any(kw in fname for kw in ["raw", "detail", "values", "request"]):
            discovered["raw_files"].append(fp)
        elif any(kw in fname for kw in ["stat", "transaction", "result", "summary", "action", "page"]):
            discovered["transaction_files"].append(fp)
        else:
            discovered["other_files"].append(fp)
    return discovered


def parse_all_csv(results_path, separator=None):
    discovered = discover_csv_files(results_path)
    parsed = {"transactions": [], "monitors": [], "files_processed": [], "files_skipped": []}
    for fp in discovered["transaction_files"]:
        result = parse_transaction_csv(fp, separator)
        if result:
            parsed["transactions"].extend(result)
            parsed["files_processed"].append({"file": fp, "type": "transactions", "records": len(result)})
        else:
            parsed["files_skipped"].append({"file": fp, "reason": "Unrecognized transaction format"})
    for fp in discovered["monitor_files"]:
        result = parse_monitor_csv(fp, separator)
        if result:
            parsed["monitors"].extend(result)
            parsed["files_processed"].append({"file": fp, "type": "monitors", "records": len(result)})
        else:
            parsed["files_skipped"].append({"file": fp, "reason": "Unrecognized monitor format"})
    for fp in discovered["other_files"] + discovered["raw_files"]:
        result = parse_transaction_csv(fp, separator)
        if result:
            parsed["transactions"].extend(result)
            parsed["files_processed"].append({"file": fp, "type": "transactions", "records": len(result)})
            continue
        result = parse_monitor_csv(fp, separator)
        if result:
            parsed["monitors"].extend(result)
            parsed["files_processed"].append({"file": fp, "type": "monitors", "records": len(result)})
            continue
        parsed["files_skipped"].append({"file": fp, "reason": "Could not determine format"})
    return parsed


def build_monitor_summary(monitors):
    monitor_summary = {}
    for entry in monitors:
        host = entry.get("host", "Unknown")
        metric = entry.get("metric", "Unknown")
        value = entry.get("value", 0)
        if host not in monitor_summary:
            monitor_summary[host] = {}
        if metric not in monitor_summary[host]:
            monitor_summary[host][metric] = {"values": [], "avg": 0, "max": 0}
        monitor_summary[host][metric]["values"].append(value)
    for host in monitor_summary:
        for metric in monitor_summary[host]:
            values = monitor_summary[host][metric]["values"]
            monitor_summary[host][metric]["avg"] = round(sum(values) / len(values), 2) if values else 0
            monitor_summary[host][metric]["max"] = round(max(values), 2) if values else 0
            del monitor_summary[host][metric]["values"]
    return monitor_summary


# ---------------------------------------------------------------------------
# HTML parsing helpers
# ---------------------------------------------------------------------------

class TableParser(HTMLParser):
    """Extract table data from NeoLoad HTML reports."""

    def __init__(self):
        super().__init__()
        self.tables = []
        self._current_table = None
        self._current_row = None
        self._current_cell = None
        self._in_cell = False
        self._in_div = False
        self._div_text = ""

    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        if tag == "table":
            self._current_table = {"class": attrs_dict.get("class", ""), "rows": []}
        elif tag == "tr" and self._current_table is not None:
            self._current_row = []
        elif tag in ("td", "th") and self._current_row is not None:
            self._current_cell = ""
            self._in_cell = True
        elif tag == "div" and self._in_cell:
            self._in_div = True
            self._div_text = ""

    def handle_endtag(self, tag):
        if tag == "table" and self._current_table is not None:
            self.tables.append(self._current_table)
            self._current_table = None
        elif tag == "tr" and self._current_row is not None and self._current_table is not None:
            self._current_table["rows"].append(self._current_row)
            self._current_row = None
        elif tag in ("td", "th") and self._in_cell:
            self._in_cell = False
            if self._current_row is not None:
                text = re.sub(r'\s+', ' ', self._current_cell.strip())
                self._current_row.append(text)
            self._current_cell = None
        elif tag == "div" and self._in_div:
            self._in_div = False
            if self._in_cell and self._current_cell is not None:
                self._current_cell += self._div_text

    def handle_data(self, data):
        if self._in_div and self._in_cell:
            self._div_text += data
        elif self._in_cell and self._current_cell is not None:
            self._current_cell += data


def safe_float_html(value, default=0.0):
    if not value or value.strip() in ("-", ""):
        return default
    try:
        cleaned = value.strip().replace(",", "").replace("%", "").replace("\xa0", "")
        return float(cleaned) if cleaned else default
    except (ValueError, TypeError):
        return default


def parse_summary_html(html_content):
    parser = TableParser()
    parser.feed(html_content)
    result = {
        "name": "", "project": "", "scenario": "", "status": "Unknown",
        "start_date": "", "end_date": "", "duration": "", "load_policy": "",
        "lg_hosts": "", "statistics": {}, "general_stats": {},
        "transaction_stats": {}, "errors": [], "alerts": {},
    }
    for table in parser.tables:
        cls = table.get("class", "")

        if "simpleResultSummary" in cls:
            for row in table["rows"]:
                for i, cell in enumerate(row):
                    if i + 1 >= len(row):
                        continue
                    if cell == "Name":        result["name"]       = row[i + 1]
                    elif cell == "Project":   result["project"]    = row[i + 1]
                    elif cell == "Scenario":  result["scenario"]   = row[i + 1]
                    elif cell == "Start date": result["start_date"] = row[i + 1]
                    elif cell == "End date":  result["end_date"]   = row[i + 1]
                    elif cell == "Duration":  result["duration"]   = row[i + 1]
                    elif cell == "LG Hosts":  result["lg_hosts"]   = row[i + 1]
                    elif cell == "Status":
                        t = row[i + 1]
                        result["status"] = "Passed" if ("Passed" in t or "PASS" in t) else "Failed"

        if "statistics_summary" in cls:
            for row in table["rows"]:
                for i, cell in enumerate(row):
                    if i + 1 >= len(row):
                        continue
                    s = result["statistics"]
                    if "Total pages" in cell:          s["total_pages"]               = safe_float_html(row[i + 1])
                    elif "Total requests" in cell:     s["total_requests"]            = safe_float_html(row[i + 1])
                    elif "Total users launched" in cell: s["total_users"]             = safe_float_html(row[i + 1])
                    elif "Total iterations" in cell:   s["total_iterations"]          = safe_float_html(row[i + 1])
                    elif "Total throughput" in cell:   s["total_throughput"]          = row[i + 1].strip()
                    elif "Total request errors" in cell: s["total_errors"]            = safe_float_html(row[i + 1])
                    elif "Average pages/s" in cell:    s["avg_pages_per_sec"]         = safe_float_html(row[i + 1])
                    elif "Average requests/s" in cell: s["avg_requests_per_sec"]      = safe_float_html(row[i + 1])
                    elif "Average Request response" in cell:
                        s["avg_request_response_time_s"] = safe_float_html(row[i + 1].replace("s", "").strip())
                    elif "Average Page response" in cell:
                        s["avg_page_response_time_s"]    = safe_float_html(row[i + 1].replace("s", "").strip())
                    elif "Error rate" in cell:         s["error_rate_pct"]            = safe_float_html(row[i + 1])
                    elif "Average throughput" in cell: s["avg_throughput"]            = row[i + 1].strip()

        if "all_statistics_content" in cls:
            current_label = None
            for row in table["rows"]:
                if not row:
                    continue
                if row[0] in ("Min", ""):
                    continue
                if len(row) == 1 or any(kw in row[0] for kw in ["All User Paths", "All pages", "All requests", "All Transactions"]):
                    current_label = row[0].strip()
                    continue
                if current_label and len(row) >= 6:
                    stats = {
                        "min": safe_float_html(row[0]), "avg": safe_float_html(row[1]),
                        "max": safe_float_html(row[2]), "count": safe_float_html(row[3]),
                        "errors": safe_float_html(row[4]), "error_pct": safe_float_html(row[5]),
                    }
                    if len(row) >= 11:
                        stats.update({
                            "perc_50": safe_float_html(row[6]), "perc_95": safe_float_html(row[7]),
                            "perc_99": safe_float_html(row[8]), "std_dev": safe_float_html(row[9]),
                        })
                    if "Transaction" in current_label:
                        result["transaction_stats"] = stats
                    else:
                        result["general_stats"][current_label] = stats
                    current_label = None

        if "errors_summary" in cls:
            for row in table["rows"]:
                if len(row) >= 3 and row[0] not in ("Error Type", "No errors", ""):
                    result["errors"].append({"type": row[0], "count": safe_float_html(row[1]), "description": row[2]})

    return result


def parse_transactions_html(html_content):
    parser = TableParser()
    parser.feed(html_content)
    transactions = []
    current_user_path = None
    for table in parser.tables:
        cls = table.get("class", "")
        if "hierarchical" not in cls or "shared" in cls:
            continue
        for row in table["rows"]:
            if not row or len(row) < 7:
                continue
            if row[0] in ("", "Min") or (len(row) > 1 and row[1] in ("Min",)):
                continue
            name = row[0].strip()
            if not name or name in ("Init", "End", "Actions"):
                continue
            min_val   = safe_float_html(row[1])
            avg_val   = safe_float_html(row[2])
            max_val   = safe_float_html(row[3])
            count_val = safe_float_html(row[4])
            has_percentiles = len(row) >= 10 and row[7].strip() not in ("-", "")
            if not has_percentiles and min_val > 500:
                current_user_path = name
                continue
            txn = {
                "element":      name,
                "user_path":    current_user_path or "",
                "min":          min_val,
                "avg":          avg_val,
                "max":          max_val,
                "count":        count_val,
                "failure":      safe_float_html(row[5]) if len(row) > 5 else 0,
                "failure_rate": safe_float_html(row[6]) if len(row) > 6 else 0,
            }
            if len(row) >= 12:
                txn["perc_50"] = safe_float_html(row[7])
                txn["perc_95"] = safe_float_html(row[8])
                txn["perc_99"] = safe_float_html(row[9])
                txn["std_dev"] = safe_float_html(row[10])
            txn["success"]      = txn["count"] - txn["failure"]
            txn["success_rate"] = round((txn["success"] / txn["count"] * 100), 1) if txn["count"] > 0 else 100
            transactions.append(txn)
    return transactions


# ---------------------------------------------------------------------------
# Format detection
# ---------------------------------------------------------------------------

def detect_format(path):
    """Return 'html' or 'csv' based on what's present at the path."""
    p = Path(path)
    if p.is_file():
        return "csv"
    html_files = (p / "summary.html").exists() or (p / "transactions.html").exists()
    if html_files:
        return "html"
    # Recurse one level for summary files
    for child in p.iterdir():
        if child.is_dir():
            if (child / "summary.html").exists() or (child / "transactions.html").exists():
                return "html"
    csv_files = list(p.glob("*.csv")) + list(p.glob("**/*.csv"))
    if csv_files:
        return "csv"
    return "html"  # default to HTML if nothing conclusive


# ---------------------------------------------------------------------------
# Unified output builder
# ---------------------------------------------------------------------------

def build_output(parsed_transactions, project_name, test_date, source_path,
                 label=None, infrastructure=None, summary_meta=None,
                 files_processed=None, files_skipped=None):
    """Build the structured output JSON (same schema regardless of input format)."""
    transactions = parsed_transactions
    total_executions = sum(t.get("count", 0) for t in transactions)
    total_success    = sum(t.get("success", 0) for t in transactions)
    total_failure    = sum(t.get("failure", 0) for t in transactions)
    overall_error_rate = (total_failure / total_executions * 100) if total_executions > 0 else 0

    avg_rt  = [t["avg"]     for t in transactions if t.get("avg", 0)     > 0]
    p95_vals = [t["perc_95"] for t in transactions if t.get("perc_95", 0) > 0]
    max_vals = [t["max"]     for t in transactions if t.get("max", 0)     > 0]

    meta = summary_meta or {}
    stats = meta.get("statistics", {})

    output = {
        "project_name":  project_name,
        "test_date":     test_date,
        "test_label":    label or meta.get("name", ""),
        "parsed_at":     datetime.now().isoformat(),
        "source_path":   str(source_path),
        "test_metadata": {
            "name":        meta.get("name", ""),
            "scenario":    meta.get("scenario", ""),
            "status":      meta.get("status", ""),
            "start_date":  meta.get("start_date", ""),
            "end_date":    meta.get("end_date", ""),
            "duration":    meta.get("duration", ""),
            "lg_hosts":    meta.get("lg_hosts", ""),
            "total_users": stats.get("total_users", 0),
            "load_policy": meta.get("load_policy", ""),
        },
        "summary": {
            "total_transaction_types":      len(transactions),
            "total_executions":             total_executions,
            "total_success":                total_success,
            "total_failure":                total_failure,
            "overall_error_rate":           round(overall_error_rate, 2),
            "avg_response_time":            round(sum(avg_rt) / len(avg_rt), 2) if avg_rt else 0,
            "max_response_time":            round(max(max_vals), 2) if max_vals else 0,
            "p95_response_time":            round(max(p95_vals), 2) if p95_vals else 0,
            "total_pages":                  stats.get("total_pages", 0),
            "total_requests":               stats.get("total_requests", 0),
            "avg_pages_per_sec":            stats.get("avg_pages_per_sec", 0),
            "avg_requests_per_sec":         stats.get("avg_requests_per_sec", 0),
            "avg_page_response_time_s":     stats.get("avg_page_response_time_s", 0),
            "avg_request_response_time_s":  stats.get("avg_request_response_time_s", 0),
            "error_rate_pct":               stats.get("error_rate_pct", 0),
            "total_throughput":             stats.get("total_throughput", ""),
            "avg_throughput":               stats.get("avg_throughput", ""),
        },
        "transactions":     sorted(transactions, key=lambda t: t.get("avg", 0), reverse=True),
        "infrastructure":   infrastructure or {},
        "errors":           meta.get("errors", []),
        "general_stats":    meta.get("general_stats", {}),
        "transaction_stats": meta.get("transaction_stats", {}),
        "files_processed":  files_processed or [],
        "files_skipped":    files_skipped or [],
    }
    return output


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Parse NeoLoad performance test results (HTML or CSV)")
    parser.add_argument("results_path", help="Path to NeoLoad results directory or CSV file")
    parser.add_argument("--project",    required=True, help="Project/application name")
    parser.add_argument("--date",       required=True, help="Test execution date (YYYY-MM-DD)")
    parser.add_argument("--label",      default=None,  help="Optional label for this run (e.g. 'No background processes')")
    parser.add_argument("--separator",  default=None,  help="CSV separator (auto-detected if not specified, CSV mode only)")
    parser.add_argument("--format",     default=None,  choices=["html", "csv"],
                        help="Force input format (default: auto-detect)")
    parser.add_argument("--output-dir", default=".tmp", help="Output directory (default: .tmp)")
    parser.add_argument("--output-file", default=None, help="Specific output filename (without directory)")
    args = parser.parse_args()

    load_dotenv()

    results_path = Path(args.results_path)
    if not results_path.exists():
        print(f"Error: Results path not found: {results_path}", file=sys.stderr)
        sys.exit(1)

    fmt = args.format or detect_format(results_path)
    print(f"Parsing NeoLoad results from: {results_path}")
    print(f"Project: {args.project} | Date: {args.date} | Format: {fmt.upper()}")

    if fmt == "html":
        # Locate the directory containing summary.html
        html_dir = results_path
        if not (html_dir / "summary.html").exists():
            # Try one level deeper
            for child in sorted(results_path.iterdir()):
                if child.is_dir() and (child / "summary.html").exists():
                    html_dir = child
                    break

        summary_file = html_dir / "summary.html"
        txn_file     = html_dir / "transactions.html"

        summary_meta = {}
        if summary_file.exists():
            with open(summary_file, "r", encoding="utf-8") as f:
                summary_meta = parse_summary_html(f.read())
            print(f"  Parsed summary: {summary_meta.get('name', '(unnamed)')}")
        else:
            print("Warning: summary.html not found", file=sys.stderr)

        if txn_file.exists():
            with open(txn_file, "r", encoding="utf-8") as f:
                transactions = parse_transactions_html(f.read())
            print(f"  Parsed {len(transactions)} transactions")
        else:
            print("Error: transactions.html not found", file=sys.stderr)
            sys.exit(1)

        if not transactions:
            print("Error: No transaction data found.", file=sys.stderr)
            sys.exit(1)

        output = build_output(
            transactions, args.project, args.date, results_path,
            label=args.label, summary_meta=summary_meta,
        )

    else:  # csv
        parsed = parse_all_csv(results_path, args.separator)
        if not parsed["transactions"] and not parsed["monitors"]:
            print("Error: No parseable NeoLoad CSV data found.", file=sys.stderr)
            for s in parsed["files_skipped"]:
                print(f"  Skipped: {s['file']} — {s['reason']}")
            sys.exit(1)

        output = build_output(
            parsed["transactions"], args.project, args.date, results_path,
            label=args.label,
            infrastructure=build_monitor_summary(parsed["monitors"]),
            files_processed=parsed["files_processed"],
            files_skipped=parsed["files_skipped"],
        )

    # Save output
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    if args.output_file:
        output_file = output_dir / args.output_file
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"neoload_parsed_{timestamp}.json"

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    # Update manifest
    manifest_file = output_dir / "performance_latest.json"
    manifest = {}
    if manifest_file.exists():
        with open(manifest_file, "r") as f:
            manifest = json.load(f)
    manifest.update({
        "project_name": args.project,
        "test_date":    args.date,
        "created_at":   datetime.now().isoformat(),
        "latest_parsed": {
            "parsed_file":       str(output_file),
            "source_path":       str(results_path),
            "source_format":     fmt,
            "transaction_count": len(output["transactions"]),
            "total_records":     output["summary"]["total_executions"],
        }
    })
    with open(manifest_file, "w") as f:
        json.dump(manifest, f, indent=2)

    # Print summary
    print(f"\nParsing complete:")
    if output["test_metadata"].get("name"):
        print(f"  Test:       {output['test_metadata']['name']}")
    if output["test_metadata"].get("duration"):
        print(f"  Duration:   {output['test_metadata']['duration']}")
    print(f"  Transactions: {len(output['transactions'])}")
    print(f"  Executions:   {output['summary']['total_executions']:.0f}")
    print(f"  Error rate:   {output['summary']['overall_error_rate']:.2f}%")
    print(f"  Output:       {output_file}")
    print(f"  Manifest:     {manifest_file}")

    return {"status": "success", "data": {"output_file": str(output_file), "summary": output["summary"]}}


if __name__ == "__main__":
    result = main()
    sys.exit(0 if result["status"] == "success" else 1)
